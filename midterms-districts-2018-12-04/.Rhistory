author <- strsplit(author, "<")[[1]][1]
author <- trimws(author)
}
} else {
author <- NA
}
blogtext$author <- author
temp_date <- xpathSApply(blogdata, "//span[@class='time-wrapper']/text()", saveXML)[[1]]
blogtext$date <- ifelse(grepl("am", temp_date) | grepl("pm", temp_date), as.character(Sys.Date()),
as.character(as.Date(trimws(temp_date), format = "%B %d, %Y")))
text <- xpathSApply(blogdata, "//span[@class='storycontent']/p", saveXML)
text2 <- xpathSApply(blogdata, "//span[@class='storycontent']/div[@class='flourish-embed']/p", saveXML)
text <- c(text, text2)
text <- gsub('<.*?>', '', text, perl = T)
text <- gsub("\\n", "", text)
text <- gsub("\\s{2,}", " ", text)
text <- trimws(text)
blogtext$textlength <- sum(nchar(text))
blogtext$title <- xpathSApply(blogdata, "//h1[@class='storyheadline']/text()", saveXML)[[1]]
# get position info
pos <- xpathSApply(blogdata, "//span[@class='storybyline']", saveXML)[[1]]
pos <- strsplit(pos, ",")
pos <- trimws(gsub("</span>", "", pos[[1]][2]))
blogtext$position <- pos
}
all_text <- rbind(all_text, blogtext)
print(blogtext$title)
if ((Sys.Date() - as.Date(blogtext$date)) > 31) {
break
}
}
}
links <- vector("list", length = length(news_urls))
for (i in seq(length(news_urls)[1])) {
url <- news_urls[i]
print(url)
postdata <- htmlParse(getURI(url))
links[[i]] <- unname(xpathSApply(postdata,"//h2/a/@href"))
}
l <- length(links)*length(links[[1]])
all_text <- data.frame(title = character(),
textlength = character(),
date = character(),
author = character(),
position = character(),
stringsAsFactors = F)
slinks <- links
for (j in 1:length(slinks)) {
links <- slinks[[j]]
for (i in 1:length(links)) {
blogtext <- data.frame(title = "",
textlength = "",
date = "",
author = "",
position = "", stringsAsFactors = F)
print(i)
blogdata <- htmlParse(getURI(links[i]))
feature <- length(xpathSApply(blogdata, "//div[contains(@class, 'full-width')]"))
if (feature) {
author <- xpathSApply(blogdata, "//div[@class='storymeta']/p/a/text()", saveXML)[[1]]
blogtext$author <- author
# position
pos <- xpathSApply(blogdata, "//div[@class='storymeta']/p", saveXML)[[1]]
pos <- strsplit(pos, ",")[[1]][2]
pos <- trimws(gsub("</span>", "", pos[[1]][2]))
blogtext$position <- pos
temp_date <- xpathSApply(blogdata, "//div[@class='storymeta']/p/span/span[@class='time-wrapper']/text()", saveXML)[[1]]
blogtext$date <- ifelse(grepl("am", temp_date) | grepl("pm", temp_date), as.character(Sys.Date()),
as.character(as.Date(trimws(temp_date), format = "%B %d, %Y")))
text <- xpathSApply(blogdata, "//span[@class='storycontent']/p", saveXML)
text2 <- xpathSApply(blogdata, "//span[@class='storycontent']/div[@class='flourish-embed']/p", saveXML)
text <- c(text, text2)
text <- gsub('<.*?>', '', text, perl = T)
text <- gsub("\\n", "", text)
text <- gsub("\\s{2,}", " ", text)
text <- trimws(text)
blogtext$textlength <- sum(nchar(text))
blogtext$title <- xpathSApply(blogdata, "//*[@class='storyheadline']/text()", saveXML)[[1]]
} else {
author <- xpathSApply(blogdata, "//*[@class='storybyline']", saveXML)
if (length(author) >= 1) {
author <- author[[1]]
pos <- strsplit(author, ",")
pos <- trimws(gsub("</span>", "", pos[[1]][2]))
blogtext$position <- pos
author <- strsplit(author, "By")[[1]][2]
author <- trimws(strsplit(author, ",")[[1]][1])
if (grepl("<a", author)) {
author <- strsplit(author, ">")[[1]][2]
author <- strsplit(author, "<")[[1]][1]
author <- trimws(author)
}
} else {
author <- NA
pos <- NA
}
blogtext$author <- author
temp_date <- xpathSApply(blogdata, "//span[@class='time-wrapper']/text()", saveXML)[[1]]
blogtext$date <- ifelse(grepl("am", temp_date) | grepl("pm", temp_date), as.character(Sys.Date()),
as.character(as.Date(trimws(temp_date), format = "%B %d, %Y")))
text <- xpathSApply(blogdata, "//span[@class='storycontent']/p", saveXML)
text2 <- xpathSApply(blogdata, "//span[@class='storycontent']/div[@class='flourish-embed']/p", saveXML)
text <- c(text, text2)
text <- gsub('<.*?>', '', text, perl = T)
text <- gsub("\\n", "", text)
text <- gsub("\\s{2,}", " ", text)
text <- trimws(text)
blogtext$textlength <- sum(nchar(text))
blogtext$title <- xpathSApply(blogdata, "//h1[@class='storyheadline']/text()", saveXML)[[1]]
}
all_text <- rbind(all_text, blogtext)
print(blogtext$title)
if ((Sys.Date() - as.Date(blogtext$date)) > 31) {
break
}
}
}
blogtext
all_text
blogtext
library(rvest)
url <- "http://www.legis.state.pa.us/cfdocs/legis/home/findyourlegislator/county_list.cfm?CNTYLIST=ALLEGHENY"
s <- html_session(url)
s %>% html_nodes(xpath = "/html/body/div/section/div/div[2]/table")
s %>% html_nodes(xpath = "/html/body/div/section/div/div[2]/table") %>% html_table() -> table
table
table[[1]]$`PA House District` -> house
house
unique(house)
unique(house)
%>% order()
order(unique(house))
house[order(unique(house))]
house[order(-unique(house))]
is(house)
house[order(-unique(house))]
house[order(unique(house))]
unique(house)[order(unique(house))]
unique(house)[order(unique(house))] -> allegheny
allegheny
toJSON(allegheny)
library(jsonlite)
toJSON(allegheny)
library(data.table)
df <- fread("~/Research/enick/prod/density.dat")
df
plot(df$V1)
df$V1 <- rep(1:(nrow(df)/30), each = 30)
df
df <- fread("~/Research/enick/prod/density.dat")
df$T <- rep(1:(nrow(df)/30), each = 30)
colnames(df) <- c("p", "T")
df
plot(df$T, df$p)
library(tidyverse)
p <- ggplot(df) + geom_point(aes(x = T + 299, y = p), alpha = 0.3, colour = "red")
p
p <- ggplot(df) + geom_point(aes(x = T + 299, y = p), alpha = 0.3, colour = "red") + theme_minimal()
p
library(data.table)
df <- fread("~/Research/enick/prod/600/density.dat")
df
plot(df$V1)
df$V1
library(RCurl)
library(XML)
news_urls <- c("https://pittnews.com/article/category/opinions/columns/")
links <- vector("list", length = length(news_urls))
for (i in seq(length(news_urls)[1])) {
url <- news_urls[i]
print(url)
postdata <- htmlParse(getURI(url))
links[[i]] <- unname(xpathSApply(postdata,"//h2/a/@href"))
}
l <- length(links)*length(links[[1]])
links
all_text <- data.frame(title = character(),
textlength = character(),
date = character(),
author = character(),
position = character(),
stringsAsFactors = F)
slinks <- links
for (j in 1:length(slinks)) {
links <- slinks[[j]]
for (i in 1:length(links)) {
blogtext <- data.frame(title = "",
textlength = "",
date = "",
author = "",
position = "", stringsAsFactors = F)
print(i)
blogdata <- htmlParse(getURI(links[i]))
feature <- length(xpathSApply(blogdata, "//div[contains(@class, 'full-width')]"))
if (feature) {
author <- xpathSApply(blogdata, "//div[@class='storymeta']/p/a/text()", saveXML)[[1]]
blogtext$author <- author
# position
pos <- xpathSApply(blogdata, "//div[@class='storymeta']/p", saveXML)[[1]]
pos <- strsplit(pos, ",")[[1]][2]
pos <- trimws(gsub("</span>", "", pos[[1]][2]))
blogtext$position <- pos
temp_date <- xpathSApply(blogdata, "//div[@class='storymeta']/p/span/span[@class='time-wrapper']/text()", saveXML)[[1]]
blogtext$date <- ifelse(grepl("am", temp_date) | grepl("pm", temp_date), as.character(Sys.Date()),
as.character(as.Date(trimws(temp_date), format = "%B %d, %Y")))
text <- xpathSApply(blogdata, "//span[@class='storycontent']/p", saveXML)
text2 <- xpathSApply(blogdata, "//span[@class='storycontent']/div[@class='flourish-embed']/p", saveXML)
text <- c(text, text2)
text <- gsub('<.*?>', '', text, perl = T)
text <- gsub("\\n", "", text)
text <- gsub("\\s{2,}", " ", text)
text <- trimws(text)
blogtext$textlength <- sum(nchar(text))
blogtext$title <- xpathSApply(blogdata, "//*[@class='storyheadline']/text()", saveXML)[[1]]
} else {
author <- xpathSApply(blogdata, "//*[@class='storybyline']", saveXML)
if (length(author) >= 1) {
author <- author[[1]]
pos <- strsplit(author, ",")
pos <- trimws(gsub("</span>", "", pos[[1]][2]))
blogtext$position <- pos
author <- strsplit(author, "By")[[1]][2]
author <- trimws(strsplit(author, ",")[[1]][1])
if (grepl("<a", author)) {
author <- strsplit(author, ">")[[1]][2]
author <- strsplit(author, "<")[[1]][1]
author <- trimws(author)
}
} else {
author <- NA
pos <- NA
}
blogtext$author <- author
temp_date <- xpathSApply(blogdata, "//span[@class='time-wrapper']/text()", saveXML)[[1]]
blogtext$date <- ifelse(grepl("am", temp_date) | grepl("pm", temp_date), as.character(Sys.Date()),
as.character(as.Date(trimws(temp_date), format = "%B %d, %Y")))
text <- xpathSApply(blogdata, "//span[@class='storycontent']/p", saveXML)
text2 <- xpathSApply(blogdata, "//span[@class='storycontent']/div[@class='flourish-embed']/p", saveXML)
text <- c(text, text2)
text <- gsub('<.*?>', '', text, perl = T)
text <- gsub("\\n", "", text)
text <- gsub("\\s{2,}", " ", text)
text <- trimws(text)
blogtext$textlength <- sum(nchar(text))
blogtext$title <- xpathSApply(blogdata, "//h1[@class='storyheadline']/text()", saveXML)[[1]]
}
all_text <- rbind(all_text, blogtext)
print(blogtext$title)
if ((Sys.Date() - as.Date(blogtext$date)) > 31) {
break
}
}
}
all_text
write.csv(df, "~/Desktop/op-pay-october.csv", row.names = F)
write.csv(all_text, "~/Desktop/op-pay-october.csv", row.names = F)
df <- all_text
library(dplyr)
df %>% group_by(author) %>% summarize(n = sum(textlength))
df %>% group_by(author) %>% summarize(n = sum(textlength)) -> df
df
22403*0.0033
write.csv(df, "~/op-pay-october-by-writer.csv", row.names = F)
write.csv(df, "~/Desktop/op-pay-october-by-writer.csv", row.names = F)
source('~/pitt-news-graphics/scripts/odds.R')
today
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
today
source('~/pitt-news-graphics/scripts/odds.R')
paste("~/pitt-news-graphics/2018-2019", month(today), day(today), sep="/")
source('~/pitt-news-graphics/scripts/odds.R')
df
df
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
df
df %>% group_by(game)
df %>% group_by(game) %>% summarize()
source('~/pitt-news-graphics/scripts/odds.R')
df
source('~/pitt-news-graphics/scripts/odds.R')
df
source('~/pitt-news-graphics/scripts/odds.R')
df
source('~/pitt-news-graphics/scripts/odds.R')
p
source('~/pitt-news-graphics/scripts/odds.R')
p
source('~/pitt-news-graphics/scripts/odds.R')
p
source('~/pitt-news-graphics/scripts/odds.R')
p
?labs
source('~/pitt-news-graphics/scripts/odds.R')
p
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
?scale_fill_manual
source('~/pitt-news-graphics/scripts/odds.R')
df$fill
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
?geom_text
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
dev.off()
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/scrape_candidates.R')
table
s %>% html_nodes(xpath = "/html/body/div[7]/div[3]/div/div/div[2]/div[4]/div[6]/div[1]/table")
table <- s %>% html_nodes("#General_election > table:nth-child(1)") %>% html_table()
table
s %>% html_nodes("#General_election > table:nth-child(1)")
s %>% html_nodes("#General_election")
s %>% html_nodes("#General_election table")
s %>% html_nodes("div#General_election")
s %>% html_nodes("#General_election")
url <- "https://ballotpedia.org/Pennsylvania_House_of_Representatives_elections,_2018"
s <- html_session(url)
%>% html_nodes("#General_election")
s  %>% html_nodes("#General_election")
table <- s %>% html_nodes("div#General_election") %>% html_table()
table
s %>% html_nodes("div#General_election")
s %>% html_nodes("#headertabs")
s %>% html_nodes("#headertabs")
s %>% html_nodes(css = "#headertabs")
url %>% html %>% html_nodes(css = "#headertabs")
url %>% read_html %>% html_nodes(css = "#headertabs")
html <- url %>% read_html
html
html %>% html_nodes("#General_election")
text_split <- strsplit(text, "\n")
source('~/.active-rstudio-document')
text_split
source('~/.active-rstudio-document')
text_split
source('~/.active-rstudio-document')
text
source('~/.active-rstudio-document')
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/scripts/odds.R')
source('~/.active-rstudio-document')
df
source('~/.active-rstudio-document')
p
unique(df$Date)
unique(df$Date) %>% length
nroww(df)
771/365
df$Date
df$AQS_PARAMETER_CODE
df$AQS_PARAMETER_DESC
sapply(df, unique)
?RColorBrewer
source('~/.active-rstudio-document')
p
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
?if
f
source('~/.active-rstudio-document')
p
source('~/.active-rstudio-document')
p
source('~/.active-rstudio-document')
p
source('~/.active-rstudio-document')
p
source('~/.active-rstudio-document')
p
?aggregate
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
df$wk
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
p
source('~/.active-rstudio-document')
p
source('~/.active-rstudio-document')
df
source('~/.active-rstudio-document')
p
source('~/.active-rstudio-document')
p
source('~/.active-rstudio-document')
p
source('~/.active-rstudio-document')
p
?geom_point
source('~/.active-rstudio-document')
p
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
df$DAILY_AQI_VALUE > 50
df$DAILY_AQI_VALUE > 50 %>% sum
(df$DAILY_AQI_VALUE > 50) %>% sum
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
yrs <- 2014:2018
yrs
yrs <- 2014:2018
df <- lapply(yrs, function(yr) {
return(fread(paste0("~/Desktop/aqi-data/aqi-", yr, ".csv"))
})
df <- fread("~/Desktop/ad_viz_plotval_data(1).csv")
df$Date <- mdy(df$Date)
df <- df[!duplicated(df$Date), ]
df$wk <- week(df$Date)
color_scale <- c(H = "maroon",
VU = "purple",
U = "red",
USG = "orange",
M = "yellow",
G = "green")
# df <- aggregate(DAILY_AQI_VALUE~wk, FUN=mean, data=df, na.rm=TRUE)
df$CLASS <- ifelse(df$DAILY_AQI_VALUE > 300, "H",
ifelse(df$DAILY_AQI_VALUE > 200, "VU",
ifelse(df$DAILY_AQI_VALUE > 150, "U",
ifelse(df$DAILY_AQI_VALUE > 100, "USG",
ifelse(df$DAILY_AQI_VALUE > 50, "M", "G")))))
# df$wk <- as.Date(paste0("2017", "-", df$wk, "-", 10), format = "%Y-%U-%u")
p <- ggplot(df) + geom_point(
aes(x = Date, y = DAILY_AQI_VALUE, colour = CLASS)) +
scale_colour_manual(values = color_scale) +
theme_dark() +
guides(colour = F) +
xlab("") + ylab("Air quality index (AQI)") +
labs(title = "Daily AQI during 2017",
subtitle = "The daily average AQI occasionally surpassed 50, or moderate air quality",
caption = "Source: EPA")
ggsave("~/Desktop/aqi-2017.png", p, width = 11, height = 7, dpi = 300)
df <- lapply(yrs, function(yr) {
return(fread(paste0("~/Desktop/aqi-data/aqi-", yr, ".csv")))
})
df
df <- do.call(rbind, df)
df
source('~/.active-rstudio-document')
p
max(df$DAILY_AQI_VALUE)
df$`Daily Mean PM2.5 Concentration`
df$`Daily Mean PM2.5 Concentration` %>% max
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/pitt-news-graphics/scripts/odds.R')
source('~/pitt-news-graphics/2018-2019/12/04/elections/mapping.R')
source('~/pitt-news-graphics/2018-2019/12/04/elections/mapping.R')
new_districts %>% group_by(group) %>% summarise(lng = mean(long), lat = mean(lat))
new_districts %>% group_by(group) %>% summarise(lng = mean(long), lat = mean(lat)) -> centroids_new
centroids_new
centroids_new$lng
source('~/pitt-news-graphics/2018-2019/12/04/elections/mapping.R')
source('~/pitt-news-graphics/2018-2019/12/04/elections/mapping.R')
centroids_old$group
centroids_old <- old_districts %>% group_by(id) %>% summarise(lat = mean(lat), long = mean(long))
centroids_new <- new_districts %>% group_by(id) %>% summarise(lat = mean(lat), long = mean(long))
centroids_old$group
centroids_old$id
centroids_new$id
source('~/pitt-news-graphics/2018-2019/12/04/elections/mapping.R')
old_districts$id
old_districts$group
old_districts$group %>% levels
source('~/pitt-news-graphics/2018-2019/12/04/elections/mapping.R')
source('~/pitt-news-graphics/2018-2019/12/04/elections/mapping.R')
?geom_path
source('~/pitt-news-graphics/2018-2019/12/04/elections/mapping.R')
source('~/pitt-news-graphics/2018-2019/12/04/elections/mapping.R')
source('~/pitt-news-graphics/2018-2019/12/04/elections/mapping.R')
source('~/pitt-news-graphics/2018-2019/12/04/elections/mapping.R')
source('~/pitt-news-graphics/2018-2019/12/04/elections/mapping.R')
source('~/pitt-news-graphics/2018-2019/12/04/elections/mapping.R')
old_districts
old_districts$id %>% unique
source('~/pitt-news-graphics/2018-2019/12/04/elections/mapping.R')
